import comfy
import json
import os
import time
import tempfile
import shutil
from pathlib import Path
from typing import Dict, Any, Tuple, Optional, List
import unicodedata
import re
import hashlib

# ---------- è¾…åŠ©å‡½æ•° ----------
def zh(label: str, description: str = None) -> Dict[str, str]:
    """ä¸ºå‚æ•°æ·»åŠ ä¸­æ–‡æ ‡ç­¾å’Œæè¿°"""
    return {"zh_label": label, "zh_description": description or label}

def normalize_filename(filename: str) -> str:
    """è§„èŒƒåŒ–æ–‡ä»¶åï¼Œå»é™¤ç‰¹æ®Šå­—ç¬¦"""
    return unicodedata.normalize('NFKD', filename).encode('ascii', 'ignore').decode('ascii')

def safe_getattr(obj, attr_name, default=None):
    """å®‰å…¨è·å–å¯¹è±¡å±æ€§"""
    return getattr(obj, attr_name, default) if obj else default

def clean_path(path: str) -> str:
    """æ¸…ç†è·¯å¾„å­—ç¬¦ä¸²ï¼Œå»é™¤å¤šä½™çš„å¼•å·"""
    # å»é™¤ä¸¤ç«¯çš„ç©ºæ ¼å’Œå¼•å·
    cleaned = path.strip().strip('"').strip("'")
    
    # å¤„ç†Windowsè·¯å¾„ä¸­çš„åŒåæ–œæ 
    if os.name == 'nt':
        # æ›¿æ¢åŒåæ–œæ ä¸ºå•åæ–œæ 
        cleaned = cleaned.replace('\\\\', '\\')
        # å¤„ç†è·¯å¾„å¼€å¤´å¯èƒ½çš„å¤šä½™å¼•å·
        if cleaned.startswith('"') or cleaned.startswith("'"):
            cleaned = cleaned[1:]
        if cleaned.endswith('"') or cleaned.endswith("'"):
            cleaned = cleaned[:-1]
    
    cleaned = os.path.normpath(cleaned)
    
    return cleaned

# ---------- æ ¸å¿ƒè½¬æ¢å™¨ ----------
class DoclingConverter:
    @classmethod
    def INPUT_TYPES(cls) -> Dict[str, Any]:
        return {
            "required": {
                "file_path": ("STRING", {"default": "", "multiline": False, "label": "æ–‡ä»¶è·¯å¾„", **zh("æ–‡ä»¶è·¯å¾„", "æ–‡æ¡£çš„å®Œæ•´è·¯å¾„æˆ–URL")}),
                "format": (["auto", "pdf", "docx", "pptx", "html", "markdown"], 
                          {"default": "auto", "label": "æ–‡æ¡£æ ¼å¼", **zh("æ–‡æ¡£æ ¼å¼", "è‡ªåŠ¨æ£€æµ‹æˆ–æŒ‡å®šæ ¼å¼")}),
                "auto_language": ("BOOLEAN", {"default": True, "label": "è‡ªåŠ¨æ£€æµ‹è¯­è¨€", **zh("è‡ªåŠ¨æ£€æµ‹è¯­è¨€", "å¯ç”¨OCRè¯­è¨€è‡ªåŠ¨æ£€æµ‹")}),
                "chunking": ("BOOLEAN", {"default": True, "label": "æ™ºèƒ½åˆ†å—", **zh("æ™ºèƒ½åˆ†å—", "ä¸ºRAGä¼˜åŒ–å¯ç”¨åˆ†å—å¤„ç†")}),
                "enhancements": ("BOOLEAN", {"default": True, "label": "å¢å¼ºå¤„ç†", **zh("å¢å¼ºå¤„ç†", "å¯ç”¨ä»£ç /å…¬å¼/å›¾ç‰‡å¢å¼º")}),
                "enable_cleaning": ("BOOLEAN", {"default": True, "label": "å¯ç”¨æ•°æ®æ¸…æ´—", **zh("å¯ç”¨æ•°æ®æ¸…æ´—", "æ¸…ç†å’Œç»“æ„åŒ–æ•°æ®")}),
            },
            "optional": {
                "max_pages": ("INT", {"default": 0, "min": 0, "max": 1000, "label": "æœ€å¤§é¡µæ•°", **zh("æœ€å¤§é¡µæ•°", "0=æ— é™åˆ¶")}),
                "output_path": ("STRING", {"default": "", "label": "ä¿å­˜è·¯å¾„", **zh("ä¿å­˜è·¯å¾„", "å°†JSONä¿å­˜åˆ°æŒ‡å®šè·¯å¾„")}),
                "custom_options": ("JSON", {"default": "{}", "label": "è‡ªå®šä¹‰é€‰é¡¹", **zh("è‡ªå®šä¹‰é€‰é¡¹", "JSONæ ¼å¼çš„é«˜çº§è®¾ç½®")}),
                "chunk_size": ("INT", {"default": 512, "min": 64, "max": 2048, "label": "åˆ†å—å¤§å°", **zh("åˆ†å—å¤§å°", "å­—ç¬¦æ•°/åˆ†è¯æ•°")}),
                "chunk_overlap": ("INT", {"default": 50, "min": 0, "max": 256, "label": "åˆ†å—é‡å ", **zh("åˆ†å—é‡å ", "å—é—´é‡å å­—ç¬¦æ•°")}),
                # æ–°å¢å‚æ•°ï¼šå¯ç”¨äº§å“IDç”Ÿæˆ
                "enable_product_id": ("BOOLEAN", {"default": True, "label": "å¯ç”¨äº§å“ID", **zh("å¯ç”¨äº§å“ID", "ç”Ÿæˆæ ‡å‡†åŒ–äº§å“æ ‡è¯†")}),
                # æ–°å¢å‚æ•°ï¼šå¯ç”¨æ•°å€¼ç´¢å¼•
                "enable_numerical_index": ("BOOLEAN", {"default": True, "label": "å¯ç”¨æ•°å€¼ç´¢å¼•", **zh("å¯ç”¨æ•°å€¼ç´¢å¼•", "ä¸ºæ•°å€¼å‚æ•°åˆ›å»ºç´¢å¼•åŒºé—´")}),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "STRING")
    RETURN_NAMES = ("json_output", "status", "metadata")
    FUNCTION = "convert"
    CATEGORY = "Docling/æ–‡æ¡£å¤„ç†"
    OUTPUT_NODE = True

    def convert(self, **kwargs) -> Tuple[str, str, str]:
        """æ‰§è¡Œæ–‡æ¡£è½¬æ¢"""
        start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
        try:
            # æå–å‚æ•°å¹¶æ¸…ç†è·¯å¾„
            file_path = clean_path(kwargs.get("file_path", ""))
            output_path = clean_path(kwargs.get("output_path", ""))
            
            # éªŒè¯æ–‡ä»¶å­˜åœ¨
            if not self.validate_file(file_path):
                return self.error_response(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            # å‡†å¤‡è½¬æ¢
            converter, format = self.prepare_converter(kwargs, file_path)
            
            # æ‰§è¡Œè½¬æ¢
            result = self.perform_conversion(converter, file_path)
            
            # å¤„ç†ç»“æœ
            json_output, metadata = self.process_result(result, kwargs, start_time)
            
            # ä¿å­˜æ–‡ä»¶
            save_status = self.save_output(json_output, output_path, file_path)
            
            return (json_output, f"æˆåŠŸ{save_status}", metadata)
        except Exception as e:
            return self.error_response(f"è½¬æ¢å¤±è´¥: {str(e)}") 

    def validate_file(self, file_path: str) -> bool:
        """éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        return os.path.exists(file_path)    
    
    def prepare_converter(self, params: Dict[str, Any], file_path: str):
        from docling.datamodel.pipeline_options import PdfPipelineOptions
        from docling.datamodel.base_models import InputFormat
        
        # åˆ›å»ºæµæ°´çº¿é€‰é¡¹
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = True
        pipeline_options.ocr_options.lang = ["auto"] if params.get("auto_language") else ["en"]
        pipeline_options.do_table_structure = True
        pipeline_options.generate_page_images = True
        pipeline_options.do_code_enrichment = params.get("enhancements", True)
        pipeline_options.do_formula_enrichment = params.get("enhancements", True)
        pipeline_options.do_picture_classification = params.get("enhancements", True)
        
        # æ·»åŠ è‡ªå®šä¹‰é€‰é¡¹
        if custom_options := params.get("custom_options"):
            custom_options = json.loads(custom_options)
            for key, value in custom_options.items():
                setattr(pipeline_options, key, value)
        
        # åˆ›å»ºè½¬æ¢å™¨
        from docling.document_converter import DocumentConverter, PdfFormatOption
        return (
            DocumentConverter(
                format_options={
                    InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
                }
            ),
            "pdf"
        )

    def validate_tesseract(self):
        if "TESSDATA_PREFIX" not in os.environ:
            raise EnvironmentError("TESSDATA_PREFIX environment variable not set. Refer to Docling Tesseract installation docs")
        if not shutil.which("tesseract"):
            raise FileNotFoundError("Tesseract not found in system PATH")

    def perform_conversion(self, converter, file_path: str):
        """æ‰§è¡Œæ–‡æ¡£è½¬æ¢"""
        return converter.convert(source=file_path)

    def process_result(self, result, params: Dict[str, Any], start_time: float) -> Tuple[str, str]:
        if params.get("chunking", True):
            return self.process_chunks(result.document, params, start_time)
        else:
            return self.process_full_doc(result.document, params, start_time)

    def process_chunks(self, doc, params: Dict[str, Any], start_time: float) -> Tuple[str, str]:
        """å¤„ç†åˆ†å—ç»“æœ - é€šç”¨ä¼˜åŒ–"""
        from docling.chunking import HybridChunker
        chunker = HybridChunker(
            tokenizer="BAAI/bge-small-en-v1.5",
            chunk_size=params.get("chunk_size", 512),
            overlap=params.get("chunk_overlap", 50),
            merge_peers=True
        )
        chunks = list(chunker.chunk(doc))
        
        if len(chunks) > 100:
            print(f"å¤„ç†ä¸­: å…±{len(chunks)}ä¸ªåˆ†å—...")
        
        # æ„å»ºåˆ†å—JSON
        chunk_data = []
        for chunk in chunks:
            # æå–å…ƒæ•°æ®
            chunk_meta = self.extract_chunk_metadata(chunk, params)
            
            # è·å–åŸå§‹æ–‡æœ¬å’Œå…ƒæ•°æ®
            original_text = safe_getattr(chunk, 'text', '')
            original_meta = chunk_meta
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨ä¼˜åŒ–
            if params.get("enable_cleaning", True):
                # å…ˆè¿›è¡ŒåŸºç¡€æ¸…æ´—
                cleaned_text, cleaned_meta = self.restructure_chunk_data(
                    original_text, 
                    original_meta,
                    params
                )
                
                # å¦‚æœéœ€è¦é¢å¤–é‡æ„
                if params.get("enable_restructuring", True):
                    cleaned_text, cleaned_meta = self.restructure_chunk_data(
                        cleaned_text, 
                        cleaned_meta,
                        params
                    )
            else:
                cleaned_text = original_text
                cleaned_meta = original_meta
            
            # ç§»é™¤ç©ºå­—æ®µ
            cleaned_meta = self.remove_empty_fields(cleaned_meta)
            
            chunk_data.append({
                "text": cleaned_text,
                "metadata": cleaned_meta
            })
        
        # ç”Ÿæˆè¾“å‡º
        json_output = json.dumps(chunk_data, indent=2, ensure_ascii=False)
        metadata = json.dumps({
            "chunk_count": len(chunk_data),
            "page_count": len(doc.pages) if hasattr(doc, 'pages') else 0,
            "optimizations_applied": params.get("enable_cleaning", True)
        }, ensure_ascii=False)
        
        return json_output, metadata
    
    def restructure_chunk_data(self, text: str, metadata: Dict, params: Dict[str, Any]) -> Tuple[str, Dict]:
        """é‡æ„åˆ†å—æ•°æ®ç»“æ„ - åŸºäºå±‚çº§æå–"""
        
        # 2. æ·»åŠ å…³é”®è¯
        if "keywords" not in metadata:
            metadata["keywords"] = self.extract_keywords(text)
        
        # 3. æ·»åŠ æœ€åæ›´æ–°æ—¶é—´æˆ³
        metadata["last_updated"] = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
        
        # 4. ç”Ÿæˆäº§å“IDï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if params.get("enable_product_id", True):
            metadata["product_id"] = self.generate_product_id(
                metadata.get("section", []),
                text
            )
        
        # 5. æå–æ•°å€¼å‚æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if params.get("enable_numerical_index", True):
            numerical_params = self.extract_numerical_params(text)
            if numerical_params:
                metadata["numerical_params"] = numerical_params
        
        # 6. æå–ä¿ƒé”€æ´»åŠ¨ä¿¡æ¯
        promotion_info = self.extract_promotion_info(text)
        if promotion_info:
            metadata["promotion"] = promotion_info
        
        # 7. æå–æ³¨æ„äº‹é¡¹
        warnings = self.extract_warnings(text)
        if warnings:
            metadata["warnings"] = warnings
        
        # 8. æå–é€‚ç”¨åœºæ™¯
        applicable_scenes = self.extract_applicable_scenes(text)
        if applicable_scenes:
            metadata["applicable_scenes"] = applicable_scenes
        
        return text, metadata
    
    def extract_promotion_info(self, text: str) -> List[str]:
        """æå–ä¿ƒé”€æ´»åŠ¨ä¿¡æ¯"""
        promotions = []
        
        # åŒ¹é…ä¿ƒé”€å…³é”®è¯
        promotion_patterns = [
            r'ä¼˜æƒ æ´»åŠ¨[:ï¼š]?\s*([^\n]+)',
            r'ä¿ƒé”€[:ï¼š]?\s*([^\n]+)',
            r'æ»¡(\d+)[ä»¶ç®±ä¸ªåŒ…å¥—]èµ ',
            r'è´­ä¹°(.+?)æ»¡(\d+)[ä»¶ç®±ä¸ªåŒ…å¥—]'
        ]
        
        for pattern in promotion_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                if isinstance(match, tuple):
                    # å¤„ç†å¤šä¸ªæ•è·ç»„çš„æƒ…å†µ
                    promotion = " ".join([m for m in match if m])
                else:
                    promotion = match
                
                if promotion and promotion not in promotions:
                    promotions.append(promotion)
        
        return promotions
    
    def extract_warnings(self, text: str) -> List[str]:
        """æå–æ³¨æ„äº‹é¡¹"""
        warnings = []
        
        # åŒ¹é…æ³¨æ„äº‹é¡¹å…³é”®è¯
        warning_patterns = [
            r'æ³¨æ„[:ï¼š]?\s*([^\n]+)',
            r'è­¦å‘Š[:ï¼š]?\s*([^\n]+)',
            r'ç¦å¿Œ[:ï¼š]?\s*([^\n]+)',
            r'é¿å…([^\n]+)',
            r'è¯·å‹¿([^\n]+)',
            r'ä¸åº”([^\n]+)',
            r'ä¸è¦([^\n]+)',
            r'ä¸é€‚åˆ([^\n]+)'
        ]
        
        for pattern in warning_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                if match and match not in warnings:
                    warnings.append(match)
        
        return warnings
    
    def extract_applicable_scenes(self, text: str) -> List[str]:
        """æå–é€‚ç”¨åœºæ™¯"""
        scenes = []
        
        # åŒ¹é…é€‚ç”¨åœºæ™¯å…³é”®è¯
        scene_patterns = [
            r'(?:é€‚ç”¨(?:äº|åœºæ™¯)|é€‚åˆ(?:äº|åœºæ™¯)|ç”¨äº|åº”ç”¨(?:äº|åœºæ™¯)|åœºåˆ)[:ï¼š]?\s*([^\n]+)'
        ]
        
        # åœºæ™¯åŒ–æ ‡ç­¾æå–
        scene_keywords = [
            "å®¶åº­", "å…¬å¸", "åŠå…¬å®¤", "å•†åŠ¡é…’åº—", "é«˜æ¡£é¤å…", "æœºåœº", "å…¬å›­", "å…¬å…±åœºæ‰€",
            "æ—¥æ–™åº—", "é…’æ¥¼", "æµ·é²œåº—", "å¨æˆ¿", "æµ´å®¤", "å®¶ç§", "ç¯å¢ƒæ¶ˆæ¯’", "ç‰©ä½“è¡¨é¢",
            "é¤é¥®å…·", "è¡£ç‰©", "çººç»‡å“"
        ]
        
        # å¤„ç†æ­£åˆ™åŒ¹é…çš„åœºæ™¯æè¿°
        for pattern in scene_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                if not match:
                    continue
                    
                # åˆ†å‰²å¤åˆåœºæ™¯
                split_scenes = re.split(r'[ã€,ï¼Œ;ï¼›\så’ŒåŠä¸]', match)
                for scene in split_scenes:
                    # æ¸…æ´—åœºæ™¯å­—ç¬¦ä¸²
                    scene = scene.strip().rstrip('.,:;ã€‚ï¼Œï¼šï¼›')
                    if scene and scene not in scenes:
                        scenes.append(scene)
        
        # é«˜æ•ˆæå–åœºæ™¯å…³é”®è¯
        keyword_pattern = r'\b(' + '|'.join(re.escape(kw) for kw in scene_keywords) + r')\b'
        scene_tags = list(set(re.findall(keyword_pattern, text)))
        
        # åˆå¹¶ç»“æœå¹¶å»é‡
        all_scenes = list(set(scenes + scene_tags))
        return all_scenes
    
    def __init__(self):
        # å¯é…ç½®çš„ç‰¹å¾æå–å™¨
        self.feature_extractors = [
            self.extract_key_value_pairs,
            self.extract_numerical_features,
            self.extract_entity_nouns,
            self.extract_special_features
        ]
        
        # ç‰¹å¾æƒé‡é…ç½®ï¼ˆå¯æ‰©å±•ï¼‰
        self.feature_weights = {
            "å“ç‰Œ": 10,
            "å‹å·": 9,
            "æè´¨": 8,
            "å°ºå¯¸": 7,
            "å®¹é‡": 7,
            "é‡é‡": 6,
            "é¢œè‰²": 5,
            "é€‚ç”¨åœºæ™¯": 4,
            "ä»·æ ¼": 3
        }

    def generate_product_id(self, section_hierarchy: List[str], text: str) -> str:
        """ç”Ÿæˆé€šç”¨äº§å“ID - åŸºäºæ–‡æ¡£ç»“æ„å’Œå†…å®¹ç‰¹å¾"""
        # 1. æ„å»ºå±‚çº§æ ‡è¯†
        hierarchy_id = self.generate_hierarchy_id(section_hierarchy)
        
        # 2. æå–å…³é”®ç‰¹å¾
        feature_hash = self.extract_content_features(text)
        
        # 3. ç»„åˆID
        return f"{hierarchy_id}_{feature_hash}"

    def generate_hierarchy_id(self, section_hierarchy: List[str]) -> str:
        """åŸºäºæ–‡æ¡£å±‚çº§ç”Ÿæˆæ ‡è¯†"""
        # ä½¿ç”¨æœ€åä¸¤çº§æ ‡é¢˜ä½œä¸ºåŸºç¡€
        if len(section_hierarchy) >= 2:
            # å–æœ€åä¸¤çº§æ ‡é¢˜
            last_two = section_hierarchy[-2:]
            # è§„èŒƒåŒ–æ ‡é¢˜æ–‡æœ¬
            normalized = [self.normalize_title(title) for title in last_two]
            return "_".join(normalized)
        
        # å¦‚æœå±‚çº§ä¸è¶³ï¼Œä½¿ç”¨æ‰€æœ‰å±‚çº§
        return "_".join([self.normalize_title(title) for title in section_hierarchy])

    def normalize_title(self, title: str) -> str:
        """è§„èŒƒåŒ–æ ‡é¢˜æ–‡æœ¬"""
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œç©ºæ ¼
        cleaned = re.sub(r'[^\w\u4e00-\u9fa5]', '', title)
        # é™åˆ¶é•¿åº¦
        return cleaned[:20] if len(cleaned) > 20 else cleaned

    def extract_content_features(self, text: str) -> str:
        """æå–æ–‡æœ¬å†…å®¹ç‰¹å¾å¹¶ç”Ÿæˆå“ˆå¸Œ"""
        # 1. æå–æ‰€æœ‰ç‰¹å¾
        all_features = self.extract_all_features(text)
        
        # 2. ç”Ÿæˆç‰¹å¾ç­¾å
        feature_signature = self.generate_feature_signature(all_features)
        
        # 3. ç”ŸæˆçŸ­å“ˆå¸Œ
        return self.generate_short_hash(feature_signature)

    def extract_all_features(self, text: str) -> Dict[str, List[str]]:
        """æå–æ‰€æœ‰ç±»å‹çš„ç‰¹å¾"""
        features = {}
        
        # ä½¿ç”¨æ‰€æœ‰æ³¨å†Œçš„ç‰¹å¾æå–å™¨
        for extractor in self.feature_extractors:
            feature_type = extractor.__name__.replace("extract_", "")
            result = extractor(text)
            
            if result:
                features[feature_type] = result
        
        return features

    def generate_feature_signature(self, features: Dict[str, List[str]]) -> str:
        """ç”Ÿæˆç‰¹å¾ç­¾åå­—ç¬¦ä¸²"""
        # æŒ‰æƒé‡æ’åºç‰¹å¾
        sorted_features = []
        for feature_type, values in features.items():
            # è®¡ç®—ç‰¹å¾ç±»å‹æƒé‡
            weight = self.feature_weights.get(feature_type, 1)
            sorted_features.append((feature_type, values, weight))
        
        # æŒ‰æƒé‡é™åºæ’åº
        sorted_features.sort(key=lambda x: x[2], reverse=True)
        
        # æ„å»ºç­¾åå­—ç¬¦ä¸²
        signature_parts = []
        for feature_type, values, _ in sorted_features:
            # å¯¹å€¼è¿›è¡Œæ’åºä»¥ç¡®ä¿ä¸€è‡´æ€§
            sorted_values = sorted(values)
            signature_parts.append(f"{feature_type}:{'|'.join(sorted_values)}")
        
        return "#".join(signature_parts)

    def extract_key_value_pairs(self, text: str) -> List[str]:
        """æå–é”®å€¼å¯¹ç‰¹å¾"""
        # åŒ¹é…é”®å€¼å¯¹æ¨¡å¼
        pattern = r'([\u4e00-\u9fa5a-zA-Z]{2,10})[:ï¼š]\s*([^\n]+?)(?=[\nã€,ï¼Œ;ï¼›]|$)'
        matches = re.findall(pattern, text)
        
        # è¿‡æ»¤å’Œå¤„ç†é”®å€¼å¯¹
        key_value_pairs = []
        for key, value in matches:
            # è§„èŒƒåŒ–é”®å
            normalized_key = self.normalize_feature_key(key)
            
            # æ¸…ç†å€¼
            cleaned_value = value.strip().rstrip('.,:;ã€‚ï¼Œï¼šï¼›')
            
            # è·³è¿‡ç©ºå€¼æˆ–æ— æ•ˆé”®
            if not cleaned_value or not normalized_key:
                continue
            
            # æ·»åŠ é”®å€¼å¯¹
            key_value_pairs.append(f"{normalized_key}={cleaned_value}")
        
        return key_value_pairs

    def normalize_feature_key(self, key: str) -> str:
        """è§„èŒƒåŒ–ç‰¹å¾é”®å"""
        # å¸¸è§é”®åæ˜ å°„
        key_mapping = {
            "è§„æ ¼": "å‹å·",
            "è§„æ ¼å‹å·": "å‹å·",
            "å‡€å«é‡": "å®¹é‡",
            "é‡é‡": "å‡€é‡",
            "å°ºå¯¸": "è§„æ ¼å°ºå¯¸",
            "é€‚ç”¨": "é€‚ç”¨åœºæ™¯",
            "ç”¨é€”": "é€‚ç”¨åœºæ™¯",
            "åœºæ™¯": "é€‚ç”¨åœºæ™¯",
            "å“ç‰Œ": "å“ç‰Œ",
            "æè´¨": "æè´¨",
            "é¢œè‰²": "é¢œè‰²",
            "å”®ä»·": "ä»·æ ¼",
            "ä»·æ ¼": "ä»·æ ¼",
            "åŸä»·": "ä»·æ ¼",
            "ç‰¹ä»·": "ç‰¹ä»·",
            "ä¼˜æƒ ä»·": "ç‰¹ä»·",
            "æ´»åŠ¨ä»·": "ç‰¹ä»·",
            "ç›´æ’­é—´å”®ä»·": "ç‰¹ä»·"
        }
        
        # è¿”å›æ˜ å°„å€¼æˆ–åŸé”®å
        return key_mapping.get(key, key)

    def extract_numerical_features(self, text: str) -> List[str]:
        """æå–æ•°å€¼ç‰¹å¾"""
        # åŒ¹é…æ•°å€¼æè¿°
        pattern = r'([\u4e00-\u9fa5a-zA-Z]{1,8})[:ï¼š]?\s*(\d+(?:\.\d+)?)([a-zA-Z%]{1,4})?'
        matches = re.findall(pattern, text)
        
        features = []
        for key, value, unit in matches:
            # è§„èŒƒåŒ–é”®å
            normalized_key = self.normalize_feature_key(key)
            
            # è·³è¿‡å¸¸è§éç‰¹å¾é”®
            if normalized_key in ["ç¼–å·", "é¡µç ", "åºå·", "ä»£ç "]:
                continue
            
            # æ ¼å¼åŒ–æ•°å€¼ç‰¹å¾
            feature = f"{normalized_key}:{value}"
            if unit:
                feature += unit
            features.append(feature)
        
        return features

    def extract_entity_nouns(self, text: str) -> List[str]:
        """æå–å®ä½“åè¯ç‰¹å¾"""
        # åŒ¹é…åè¯çŸ­è¯­
        pattern = r'([\u4e00-\u9fa5]{2,8}çš„)?[\u4e00-\u9fa5]{2,10}'
        matches = re.findall(pattern, text)
        
        # è¿‡æ»¤å¸¸è§è™šè¯
        stopwords = {"çš„", "æ˜¯", "åœ¨", "å’Œ", "æœ‰", "äº†", "å°±", "ä¹Ÿ", "è¦", "ä¸", "æˆ–", "ç­‰", "åŠ", "æˆ–", "ä¸”"}
        entities = []
        
        for entity in matches:
            # æ¸…ç†å®ä½“
            entity = entity.strip()
            
            # è·³è¿‡çŸ­å®ä½“å’Œè™šè¯
            if len(entity) < 2 or entity in stopwords:
                continue
            
            # è·³è¿‡é‡å¤å®ä½“
            if entity not in entities:
                entities.append(entity)
        
        return entities

    def extract_special_features(self, text: str) -> List[str]:
        """æå–ç‰¹æ®Šç‰¹å¾ï¼ˆä¼˜æƒ ã€åŠŸèƒ½ç­‰ï¼‰"""
        special_features = []
        
        # åŒ¹é…ç‰¹æ®Šç‰¹å¾æ¨¡å¼
        patterns = [
            r'(ä¼˜æƒ |æ´»åŠ¨|ä¿ƒé”€)[:ï¼š]?\s*([^\n]+)',
            r'(åŠŸèƒ½|ç‰¹ç‚¹|å–ç‚¹)[:ï¼š]?\s*([^\n]+)',
            r'(æ³¨æ„äº‹é¡¹|è­¦å‘Š|ç‰¹ä»·)[:ï¼š]?\s*([^\n]+)'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            for feature_type, value in matches:
                # æ¸…ç†å€¼
                cleaned_value = value.strip().rstrip('.,:;ã€‚ï¼Œï¼šï¼›')
                if cleaned_value:
                    special_features.append(f"{feature_type}:{cleaned_value}")
        
        return special_features

    def generate_short_hash(self, text: str, length: int = 8) -> str:
        """ç”ŸæˆçŸ­å“ˆå¸Œ"""
        # åˆ›å»ºSHA256å“ˆå¸Œ
        hash_obj = hashlib.sha256(text.encode('utf-8'))
        full_hash = hash_obj.hexdigest()
        
        # è¿”å›æŒ‡å®šé•¿åº¦çš„çŸ­å“ˆå¸Œ
        return full_hash[:length]
    
    def extract_keywords(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        # åœç”¨è¯åˆ—è¡¨
        stopwords = {"çš„", "æ˜¯", "åœ¨", "å’Œ", "æœ‰", "äº†", "å°±", "ä¹Ÿ", "è¦", "ä¸", "æˆ–", "ç­‰"}
        
        keywords = []
        
        # æå–å®Œæ•´åè¯çŸ­è¯­ï¼ˆé¿å…æˆªæ–­ï¼‰
        noun_phrases = re.findall(r'[\u4e00-\u9fa5]{2,8}(?:çš„)?[\u4e00-\u9fa5]{2,8}', text)
        keywords.extend([phrase for phrase in noun_phrases if not any(sw in phrase for sw in stopwords)])
        
        # æå–ç‰¹å®šæœ¯è¯­ï¼ˆä¸šåŠ¡å¼ºç›¸å…³è¯ï¼‰
        business_terms = re.findall(
            r'å…æ‰“å­”å®‰è£…|99%æ°¯å«é‡|[\u4e00-\u9fa5]{2,6}å‰‚|[\u4e00-\u9fa5]{2,6}æ¶²|[\u4e00-\u9fa5]{2,6}ç›’|[\u4e00-\u9fa5]{2,6}æœº', 
            text
        )
        keywords.extend(business_terms)
        
        # æå–å®Œæ•´çŸ­è¯­ï¼ˆé¿å…æˆªæ–­ï¼‰
        full_phrases = re.findall(r'[\u4e00-\u9fa5]{4,12}', text)
        keywords.extend([phrase for phrase in full_phrases if len(phrase) > 3 and not any(sw in phrase for sw in stopwords)])
        
        # å»é‡å¹¶è¿”å›
        return list(set(keywords))
    
    def extract_numerical_params(self, text: str) -> Dict[str, Any]:
        """æå–æ•°å€¼å‹å‚æ•°å¹¶å»ºç«‹ç´¢å¼•åŒºé—´"""
        params = {}
        
        # æå–å°ºå¯¸å‚æ•°
        size_matches = re.findall(r'(é•¿|å®½|é«˜|å°ºå¯¸|ä½“ç§¯|å®¹é‡|ç›´å¾„)[:ï¼š]?\s*(\d+)(mm|cm|m|L|ml)?', text)
        for dim, value, unit in size_matches:
            dim_key = f"size_{dim}"
            params[dim_key] = {
                "value": int(value),
                "unit": unit if unit else "mm",
                "range": [int(value)-5, int(value)+5]  # Â±5çš„åŒºé—´
            }
        
        # æå–æ•°é‡å‚æ•°
        quantity_match = re.search(r'(æ•°é‡|å±‚æ•°|å¼ æ•°|æŠ½æ•°)[:ï¼š]?\s*(\d+)(ä¸ª|å¼ |å±‚|æŠ½|æ”¯)?', text)
        if quantity_match:
            qty = int(quantity_match.group(2))
            params["quantity"] = {
                "value": qty,
                "range": [max(0, qty-10), qty+10] 
            }
        
        # æå–é‡é‡å‚æ•°
        weight_match = re.search(r'(å‡€å«é‡|é‡é‡|åšåº¦)[:ï¼š]?\s*(\d+)(g|kg|ä¸|)?', text)
        if weight_match:
            weight = int(weight_match.group(2))
            unit = weight_match.group(3) if weight_match.group(3) else "g"
            params["weight"] = {
                "value": weight,
                "unit": unit,
                "range": [max(0, weight-50), weight+50]  # Â±50çš„åŒºé—´
            }
        
        return params
    
    def extract_chunk_metadata(self, chunk, params: Dict[str, Any]) -> Dict[str, Any]:
        """æå–åˆ†å—å…ƒæ•°æ® - æ•´åˆä¼˜åŒ–é€»è¾‘"""
        # å®‰å…¨è·å–å±æ€§
        meta = chunk.meta
        headings = safe_getattr(meta, 'headings', [])
        item_type = safe_getattr(meta, 'item_type', 'æœªçŸ¥')
        
        # å›¾æ³¨å¤„ç†
        figure_captions = []
        if hasattr(meta, 'figure_captions'):
            for cap in meta.figure_captions:
                figure_captions.append(safe_getattr(cap, 'text', ''))
        
        # å›¾ç‰‡å…ƒæ•°æ®
        images_list = []
        if hasattr(meta, 'pictures') and meta.pictures:
            for pic in meta.pictures:
                bbox = safe_getattr(pic, 'bbox', None)
                position = [
                    safe_getattr(bbox, 'l', 0) if bbox else 0,
                    safe_getattr(bbox, 't', 0) if bbox else 0,
                    safe_getattr(bbox, 'r', 0) if bbox else 0,
                    safe_getattr(bbox, 'b', 0) if bbox else 0
                ]
                
                images_list.append({
                    "id": safe_getattr(pic, 'id', ''),
                    "description": safe_getattr(pic, 'description', ''),
                    "position": position
                })
        
        # æ„å»ºå…ƒæ•°æ®
        metadata = {
            "section": safe_getattr(meta, 'headings', []),
            "item_type": safe_getattr(meta, 'item_type', 'text'),
            "tables": [t.export_to_dict() for t in safe_getattr(meta, 'tables', [])],
            "images": images_list  # ç›´æ¥ä½¿ç”¨å®šä¹‰å¥½çš„åˆ—è¡¨
        }
        
        # å¤„ç†å¤šçº§è§„æ ¼
        if "è§„æ ¼å‹å·" in headings and item_type == "specification":
            metadata["specifications"] = self.extract_specifications(chunk.text)
        
        # ä¿ç•™éç©ºå­—æ®µ
        if figure_captions:
            metadata["figure_captions"] = figure_captions
        
        return metadata
    
    def extract_specifications(self, text: str) -> List[Dict]:
        """ä»æ–‡æœ¬ä¸­æå–è§„æ ¼ä¿¡æ¯ï¼ˆåµŒå¥—ç»“æ„ï¼‰"""
        specifications = []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…è§„æ ¼é¡¹
        spec_items = re.split(r'\n\s*-\s*', text)
        
        for item in spec_items:
            if not item.strip():
                continue
                
            # æå–è§„æ ¼å±æ€§
            spec_data = {}
            lines = item.split('\n')
            for line in lines:
                if ':' in line or 'ï¼š' in line:
                    key, value = re.split(r'[:ï¼š]', line, 1)
                    spec_data[key.strip()] = value.strip()
                elif line.strip():
                    # å¦‚æœæ²¡æœ‰åˆ†éš”ç¬¦ï¼Œå¯èƒ½æ˜¯è§„æ ¼åç§°
                    if "name" not in spec_data:
                        spec_data["name"] = line.strip()
            
            if spec_data:
                specifications.append(spec_data)
        
        return specifications
    
    def clean_text(self, text: str) -> str:
        """é€šç”¨æ–‡æœ¬æ¸…æ´—ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        # å°†\nè½¬æ¢ä¸ºåˆ—è¡¨ç»“æ„
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        
        # ç§»é™¤å°¾éƒ¨æ ‡ç‚¹
        cleaned_lines = []
        for line in lines:
            # ç§»é™¤è¡Œå°¾æ ‡ç‚¹
            line = re.sub(r'[ï¼Œ,ã€‚ã€ï¼›;]+$', '', line)
            cleaned_lines.append(line)
        
        # è¿”å›æ¸…æ´—åçš„æ–‡æœ¬ï¼ˆä¿ç•™æ¢è¡Œç¬¦ï¼‰
        return '\n'.join(cleaned_lines)
    
    def clean_metadata(self, metadata: Dict) -> Dict:
        """é€šç”¨å…ƒæ•°æ®æ¸…æ´—"""
        # ç§»é™¤ç©ºå­—æ®µ
        for field in ["figure_captions", "images"]:
            if field in metadata and not metadata[field]:
                del metadata[field]
        
        # å¡«å……ç©ºå­—æ®µ
        if "page" in metadata and not metadata["page"]:
            metadata["page"] = "æœªçŸ¥"
        
        return metadata
    
    def remove_empty_fields(self, metadata: Dict) -> Dict:
        """ç§»é™¤ç©ºå­—æ®µ"""
        # ç§»é™¤ç©ºåˆ—è¡¨å’Œç©ºå­—ç¬¦ä¸²
        for key in list(metadata.keys()):
            if metadata[key] in (None, "", [], {}):
                del metadata[key]
        return metadata
    
    def process_full_doc(self, doc, params: Dict[str, Any], start_time: float) -> Tuple[str, str]:
        """å¤„ç†å®Œæ•´æ–‡æ¡£"""
        doc_dict = doc.export_to_dict()
        doc_dict["metadata"] = {
            "source_file": params.get("file_path", "æœªçŸ¥"),  # ä½¿ç”¨ get æ–¹æ³•
            "format": params.get("format", "æœªçŸ¥"),
            "processing_time": time.time() - start_time,
            "last_updated": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
        }
        
        json_output = json.dumps(doc_dict, indent=2, ensure_ascii=False)
        metadata = json.dumps({
            "text_item_count": len(doc_dict.get("texts", [])),
            "table_count": len(doc_dict.get("tables", [])),
            "image_count": len(doc_dict.get("pictures", [])),
            "page_count": len(doc.pages) if hasattr(doc, 'pages') else 0
        }, ensure_ascii=False)
        
        return json_output, metadata
    
    def save_output(self, json_output: str, output_path: str, input_path: str) -> str:
        """ä¿å­˜è¾“å‡ºæ–‡ä»¶"""
        if not output_path:
            return ""
        
        try:
            # è§£æè¾“å‡ºè·¯å¾„
            resolved_path = self.resolve_output_path(output_path, input_path)
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(resolved_path), exist_ok=True)
            
            # å®‰å…¨å†™å…¥æ–‡ä»¶
            self.safe_write_file(resolved_path, json_output)
            
            return " | å·²ä¿å­˜"
        except Exception as e:
            return f" | ä¿å­˜å¤±è´¥: {str(e)}"
    
    def resolve_output_path(self, output_path: str, input_path: str) -> str:
        """è§£æè¾“å‡ºè·¯å¾„"""
        # å¦‚æœæ˜¯ç›®å½•ï¼Œåˆ™ç”Ÿæˆæ–‡ä»¶å
        if os.path.isdir(output_path):
            input_filename = os.path.basename(input_path)
            output_filename = f"{Path(input_filename).stem}.json"
            return os.path.join(output_path, output_filename)
        
        # å¦‚æœæ²¡æœ‰æ‰©å±•åï¼Œæ·»åŠ .json
        if not Path(output_path).suffix:
            return output_path + ".json"
        
        return output_path
    
    def safe_write_file(self, output_path: str, content: str):
        """å®‰å…¨å†™å…¥æ–‡ä»¶"""
        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', delete=False) as temp_file:
            temp_file.write(content)
            temp_path = temp_file.name
        
        try:
            # ç§»åŠ¨æ–‡ä»¶åˆ°ç›®æ ‡ä½ç½®
            shutil.move(temp_path, output_path)
        finally:
            # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«æ¸…ç†
            if os.path.exists(temp_path):
                os.remove(temp_path)
    
    def error_response(self, message: str) -> Tuple[str, str, str]:
        """ç”Ÿæˆé”™è¯¯å“åº”"""
        return (
            json.dumps({"error": message}, ensure_ascii=False),
            "å¤±è´¥",
            json.dumps({"error": message}, ensure_ascii=False)
        )

# ---------- å…ƒæ•°æ®æå–å™¨ ----------
class DoclingMetadataExtractor:
    """å…ƒæ•°æ®æå–å™¨ - ä»JSONä¸­æå–ç‰¹å®šå†…å®¹"""
    
    @classmethod
    def INPUT_TYPES(cls) -> Dict[str, Any]:
        # å®šä¹‰é€‰é¡¹åˆ—è¡¨ï¼ˆä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²åˆ—è¡¨ï¼‰
        extract_options = ["all_metadata", "text_only", "tables", "images", "product_ids", "numerical_params",
                          "promotions", "warnings", "applicable_scenes"]
        
        return {
            "required": {
                "json_input": ("STRING", {"default": "", "multiline": True, "label": "JSONè¾“å…¥", **zh("JSONè¾“å…¥", "Doclingç”Ÿæˆçš„JSONæ•°æ®")}),
                "extract_type": (extract_options, {"default": "all_metadata", "label": "æå–ç±»å‹", **zh("æå–ç±»å‹", "é€‰æ‹©è¦æå–çš„å†…å®¹ç±»å‹")}),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "LIST") 
    RETURN_NAMES = ("text_output", "metadata", "items")
    FUNCTION = "extract"
    CATEGORY = "Docling/æ–‡æ¡£å¤„ç†"

    def extract(self, json_input: str, extract_type: str) -> Tuple[str, str, list]:
        try:
            data = json.loads(json_input)
            
            # å¤„ç†åˆ†å—JSON
            if isinstance(data, list):
                return self.handle_chunked_data(data, extract_type)
            
            # å¤„ç†å®Œæ•´æ–‡æ¡£JSON
            return self.handle_full_document(data, extract_type)
                    
        except Exception as e:
            return (
                json.dumps({"error": str(e)}, ensure_ascii=False),
                "å¤±è´¥",
                []
            )
    
    def handle_chunked_data(self, data: List[Dict], extract_type: str) -> Tuple[str, str, list]:
        """å¤„ç†åˆ†å—æ•°æ®"""
        if extract_type == "text_only":
            texts = [item["text"] for item in data]
            return ("\n\n".join(texts), json.dumps({"item_count": len(texts)}), texts)
        
        elif extract_type == "tables":
            tables = [item for item in data if "table" in item.get("item_type", "")]
            return (json.dumps(tables), json.dumps({"table_count": len(tables)}), tables)
        
        elif extract_type == "images":
            images = []
            for item in data:
                if "images" in item.get("metadata", {}):
                    images.extend(item["metadata"]["images"])
            return (json.dumps(images), json.dumps({"image_count": len(images)}), images)
        
        elif extract_type == "product_ids":
            product_ids = []
            for item in data:
                metadata = item.get("metadata", {})
                if "product_id" in metadata:
                    product_ids.append({
                        "product_id": metadata["product_id"],
                        "text": item["text"][:100] + "..." if len(item["text"]) > 100 else item["text"]
                    })
            return (json.dumps(product_ids), json.dumps({"product_id_count": len(product_ids)}), product_ids)
        
        elif extract_type == "numerical_params":
            numerical_params = []
            for item in data:
                metadata = item.get("metadata", {})
                if "numerical_params" in metadata:
                    numerical_params.append({
                        "numerical_params": metadata["numerical_params"],
                        "text": item["text"][:100] + "..." if len(item["text"]) > 100 else item["text"]
                    })
            return (json.dumps(numerical_params), json.dumps({"numerical_param_count": len(numerical_params)}), numerical_params)
        
        elif extract_type == "promotions":
            promotions = []
            for item in data:
                metadata = item.get("metadata", {})
                if "promotion" in metadata:
                    for promo in metadata["promotion"]:
                        promotions.append({
                            "promotion": promo,
                            "text": item["text"][:100] + "..." if len(item["text"]) > 100 else item["text"]
                        })
            return (json.dumps(promotions), json.dumps({"promotion_count": len(promotions)}), promotions)
        
        elif extract_type == "warnings":
            warnings = []
            for item in data:
                metadata = item.get("metadata", {})
                if "warnings" in metadata:
                    for warning in metadata["warnings"]:
                        warnings.append({
                            "warning": warning,
                            "text": item["text"][:100] + "..." if len(item["text"]) > 100 else item["text"]
                        })
            return (json.dumps(warnings), json.dumps({"warning_count": len(warnings)}), warnings)
        
        elif extract_type == "applicable_scenes":
            scenes = []
            for item in data:
                metadata = item.get("metadata", {})
                if "applicable_scenes" in metadata:
                    for scene in metadata["applicable_scenes"]:
                        scenes.append({
                            "scene": scene,
                            "text": item["text"][:100] + "..." if len(item["text"]) > 100 else item["text"]
                        })
            return (json.dumps(scenes), json.dumps({"scene_count": len(scenes)}), scenes)
        
        else:  # all_metadata
            return (json.dumps(data), json.dumps({"chunk_count": len(data)}), data)
    
    def handle_full_document(self, data: Dict, extract_type: str) -> Tuple[str, str, list]:
        """å¤„ç†å®Œæ•´æ–‡æ¡£"""
        if extract_type == "text_only":
            texts = [text_item["text"] for text_item in data.get("texts", [])]
            return ("\n\n".join(texts), json.dumps({"text_count": len(texts)}), texts)
        
        elif extract_type == "tables":
            tables = data.get("tables", [])
            return (json.dumps(tables), json.dumps({"table_count": len(tables)}), tables)
        
        elif extract_type == "images":
            images = data.get("pictures", [])
            return (json.dumps(images), json.dumps({"image_count": len(images)}), images)
        
        elif extract_type == "product_ids":
            product_ids = []
            for text_item in data.get("texts", []):
                if "product_id" in text_item.get("metadata", {}):
                    product_ids.append({
                        "product_id": text_item["metadata"]["product_id"],
                        "text": text_item["text"][:100] + "..." if len(text_item["text"]) > 100 else text_item["text"]
                    })
            return (json.dumps(product_ids), json.dumps({"product_id_count": len(product_ids)}), product_ids)
        
        elif extract_type == "numerical_params":
            numerical_params = []
            for text_item in data.get("texts", []):
                if "numerical_params" in text_item.get("metadata", {}):
                    numerical_params.append({
                        "numerical_params": text_item["metadata"]["numerical_params"],
                        "text": text_item["text"][:100] + "..." if len(text_item["text"]) > 100 else text_item["text"]
                    })
            return (json.dumps(numerical_params), json.dumps({"numerical_param_count": len(numerical_params)}), numerical_params)
        
        elif extract_type == "promotions":
            promotions = []
            for text_item in data.get("texts", []):
                metadata = text_item.get("metadata", {})
                if "promotion" in metadata:
                    for promo in metadata["promotion"]:
                        promotions.append({
                            "promotion": promo,
                            "text": text_item["text"][:100] + "..." if len(text_item["text"]) > 100 else text_item["text"]
                        })
            return (json.dumps(promotions), json.dumps({"promotion_count": len(promotions)}), promotions)
        
        elif extract_type == "warnings":
            warnings = []
            for text_item in data.get("texts", []):
                metadata = text_item.get("metadata", {})
                if "warnings" in metadata:
                    for warning in metadata["warnings"]:
                        warnings.append({
                            "warning": warning,
                            "text": text_item["text"][:100] + "..." if len(text_item["text"]) > 100 else text_item["text"]
                        })
            return (json.dumps(warnings), json.dumps({"warning_count": len(warnings)}), warnings)
        
        elif extract_type == "applicable_scenes":
            scenes = []
            for text_item in data.get("texts", []):
                metadata = text_item.get("metadata", {})
                if "applicable_scenes" in metadata:
                    for scene in metadata["applicable_scenes"]:
                        scenes.append({
                            "scene": scene,
                            "text": text_item["text"][:100] + "..." if len(text_item["text"]) > 100 else text_item["text"]
                        })
            return (json.dumps(scenes), json.dumps({"scene_count": len(scenes)}), scenes)
        
        else:  # all_metadata
            return (json.dumps(data, ensure_ascii=False), json.dumps({"document_type": "full"}), [data])

# ---------- æ‰¹é‡å¤„ç†å™¨ ----------
class DoclingBatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨ - å¤„ç†æ•´ä¸ªæ–‡ä»¶å¤¹çš„æ–‡æ¡£"""
    
    @classmethod
    def INPUT_TYPES(cls) -> Dict[str, Any]:
        return {
            "required": {
                "folder_path": ("STRING", {"default": "", "label": "æ–‡ä»¶å¤¹è·¯å¾„", **zh("æ–‡ä»¶å¤¹è·¯å¾„", "åŒ…å«æ–‡æ¡£çš„æ–‡ä»¶å¤¹è·¯å¾„")}),
                "output_folder": ("STRING", {"default": "", "label": "è¾“å‡ºæ–‡ä»¶å¤¹", **zh("è¾“å‡ºæ–‡ä»¶å¤¹", "ä¿å­˜å¤„ç†ç»“æœçš„æ–‡ä»¶å¤¹")}),
                "file_types": ("STRING", {"default": "pdf,docx,pptx", "label": "æ–‡ä»¶ç±»å‹", **zh("æ–‡ä»¶ç±»å‹", "é€—å·åˆ†éš”çš„æ–‡ä»¶æ‰©å±•å")}),
            },
            "optional": {
                "converter_options": ("JSON", {"default": "{}", "label": "è½¬æ¢é€‰é¡¹", **zh("è½¬æ¢é€‰é¡¹", "JSONæ ¼å¼çš„è½¬æ¢å™¨è®¾ç½®")}),
                "max_threads": ("INT", {"default": 4, "min": 1, "max": 32, "label": "æœ€å¤§çº¿ç¨‹", **zh("æœ€å¤§çº¿ç¨‹", "åŒæ—¶å¤„ç†çš„æœ€å¤§æ–‡ä»¶æ•°")}),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("status", "summary")
    FUNCTION = "process"
    CATEGORY = "Docling/æ‰¹å¤„ç†"
    OUTPUT_NODE = True

    def process(self, folder_path: str, output_folder: str, file_types: str, 
               converter_options: str = "{}", max_threads: int = 4) -> Tuple[str, str]:
        # æ¸…ç†è·¯å¾„
        folder_path = clean_path(folder_path)
        output_folder = clean_path(output_folder)
        
        # éªŒè¯è¾“å…¥ç›®å½•
        if not os.path.isdir(folder_path):
            return (f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}", "")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_folder, exist_ok=True)
        
        # è·å–æœ‰æ•ˆæ‰©å±•å
        valid_exts = [ext.strip().lower() for ext in file_types.split(",")]
        
        # æ”¶é›†æ–‡ä»¶
        files_to_process = self.collect_files(folder_path, valid_exts)
        
        if not files_to_process:
            return ("å®Œæˆ", json.dumps({"message": "æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶"}, ensure_ascii=False))
        
        # å¤„ç†æ–‡ä»¶
        results = self.process_files(files_to_process, output_folder, converter_options, max_threads)
        
        # ç”Ÿæˆæ‘˜è¦
        summary = self.generate_summary(results)
        return ("å®Œæˆ", json.dumps(summary, indent=2, ensure_ascii=False))
    
    def collect_files(self, folder_path: str, valid_exts: List[str]) -> List[str]:
        """æ”¶é›†è¦å¤„ç†çš„æ–‡ä»¶"""
        files = []
        for filename in os.listdir(folder_path):
            file_path = os.path.join(folder_path, filename)
            if not os.path.isfile(file_path):
                continue
                
            ext = Path(filename).suffix.lower().lstrip('.')
            if not ext or ext not in valid_exts:
                continue
                
            files.append(file_path)
        return files
    
    def process_files(self, files: List[str], output_folder: str, converter_options: str, max_threads: int):
        results = []
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor(
            max_workers=max_threads,
            thread_name_prefix="DoclingWorker"
        ) as executor:
            futures = []
            for file_path in files:
                futures.append(executor.submit(
                    self.process_single_file, 
                    file_path, output_folder, converter_options
                ))
            
            for future in concurrent.futures.as_completed(futures):
                results.append(future.result())
        
        return results
    
    def process_single_file(self, file_path: str, output_folder: str, 
                           converter_options: str) -> Dict:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        filename = os.path.basename(file_path)
        output_file = os.path.join(output_folder, f"{Path(filename).stem}.json")
        
        try:
            # ä½¿ç”¨è½¬æ¢å™¨å¤„ç†æ–‡ä»¶
            converter = DoclingConverter()
            json_output, status, _ = converter.convert(
                file_path=file_path,
                format="auto",
                auto_language=True,
                chunking=True,
                enhancements=True,
                output_path=output_file,
                custom_options=converter_options
            )
            
            if "æˆåŠŸ" in status:
                return {"file": filename, "status": "æˆåŠŸ", "output": output_file}
            else:
                return {"file": filename, "status": "å¤±è´¥", "error": status}
                
        except Exception as e:
            return {"file": filename, "status": "é”™è¯¯", "error": str(e)}
    
    def generate_summary(self, results: List[Dict]) -> Dict:
        """ç”Ÿæˆå¤„ç†æ‘˜è¦"""
        success = sum(1 for r in results if r.get("status") == "æˆåŠŸ")
        failed = sum(1 for r in results if r.get("status") == "å¤±è´¥")
        errors = sum(1 for r in results if r.get("status") == "é”™è¯¯")
        
        return {
            "å¤„ç†çŠ¶æ€": "å®Œæˆ",
            "æ–‡ä»¶æ€»æ•°": len(results),
            "æˆåŠŸæ•°é‡": success,
            "å¤±è´¥æ•°é‡": failed,
            "é”™è¯¯æ•°é‡": errors,
            "ç»“æœåˆ—è¡¨": results
        }

# ---------- æ’ä»¶æ³¨å†Œ ----------
NODE_CLASS_MAPPINGS = {
    "DoclingConverter": DoclingConverter,
    "DoclingMetadataExtractor": DoclingMetadataExtractor,
    "DoclingBatchProcessor": DoclingBatchProcessor
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "DoclingConverter": "ğŸ“„æ–‡æ¡£è½¬æ¢å™¨",
    "DoclingMetadataExtractor": "ğŸ”å…ƒæ•°æ®æå–å™¨",
    "DoclingBatchProcessor": "ğŸ“‚æ‰¹é‡å¤„ç†å™¨"
}

CATEGORY_MAPPINGS = {
    "Docling/æ–‡æ¡£å¤„ç†": "æ–‡æ¡£å¤„ç†",
    "Docling/æ‰¹å¤„ç†": "æ‰¹å¤„ç†"
}

def get_custom_categories():
    """è·å–è‡ªå®šä¹‰ç±»åˆ«æ˜ å°„"""
    return CATEGORY_MAPPINGS

# æ³¨å†Œè‡ªå®šä¹‰ç±»åˆ«
comfy.utils.get_custom_categories = get_custom_categories